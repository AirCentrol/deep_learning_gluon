
cifar10分类精度对比：(草稿中)

| 模型               | 训练轮数 | 精度(%) | 每轮大致时间(s) |
| ---------------- | ---- | ----- | --------- |
| mlp              | 120  | 51.34 | 1         |
| lenet            | 50   | 69.43 | 1         |
| lenet_da         | 120  |       |           |
| resnet50         | 35   | 85.93 | 29        |
| resnet50_da      | 200  |       |           |
| wide_resnet16\*8 | 200  |       |           |

batch_size：128

GPU：GTX 1070


一些调参时的感觉和收获：

- 数据增强会减小训练准确率和测试准确率之间的差值，收敛也会变慢，不过一般会获得更好的效果。
- 从cifar-10的结果看，模型的改进能获得确实的效果。
- decay学习率时应经过足够长epochs的探索，并且不宜一次性衰减过强。

